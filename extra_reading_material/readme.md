# Lecture 1 -  Introduction to Machine Learning:

1. (7min): https://www.youtube.com/watch?v=ukzFI9rgwfU
2. (7min): AI vs Ms vs DL vs Data Science, nice discussion: https://www.youtube.com/watch?v=k2P_pHQDlp0
3. (~60min): My favorite youtube lecturer Mathematical on Machine Learning: https://www.youtube.com/watch?v=yDLKJtOVx5c&list=PLD0F06AA0D2E8FFBA

## Blogs and Text:
https://www.technologyreview.com/2018/11/17/103781/what-is-machine-learning-we-drew-you-another-flowchart/
One of the best blog entries on Introduction to Machine Learning that I have read so far: https://vas3k.com/blog/machine_learning/

# Information on statistical evaluation metrics (recall, sensitivity, RoC, confusion matrix):

1. (35min) Confusion Matrix: https://www.youtube.com/watch?v=8Oog7TXHvFY
2. Blogplost on confusion Matrix: https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/
3. RoC curves and AuC explained (Generally important): https://www.youtube.com/watch?v=OAl6eAyP-yo
4. (7 min): Accuracy, Recall and Precision https://www.youtube.com/watch?v=VPZiJGNX4_s
5. (10min) Sensitivity and Specificity: https://www.youtube.com/watch?v=vP06aMoz4v8
6. Blog Post on Accuracy, Precision and Recall: https://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/
7. Pretty amazing blog post on ML evaluation measures (must read): https://classeval.wordpress.com/introduction/basic-evaluation-measures/


# Lecture 2 - Probabilities and Naive-Bayes:

## Videos:
1. (15min) Amazing video von Bayes Theorem: https://www.youtube.com/watch?v=HZGCoVF3YvM
2. (15min) another amazing tutorial explaining concepts in probability: https://www.youtube.com/watch?v=SrEmzdOT65s
3. (7min) https://www.youtube.com/watch?v=Zt83JnjD8zg
4. (30min) Super cool video on how to use Naive Bayes for text classification)

## Blogs: 
1. https://www.investopedia.com/terms/b/bayes-theorem.asp
2. Great blog: https://betterexplained.com/articles/an-intuitive-and-short-explanation-of-bayes-theorem/
3. Another great blog: https://machinelearningmastery.com/bayes-theorem-for-machine-learning/

# Lecture 3 - Decision Trees:

## Videos:
1. (10min) https://www.youtube.com/watch?v=eKD5gxPPeY0
2. (17min) https://www.youtube.com/watch?v=7VeUPuFGJHk
3. (~60min) Amazing Lectures series with short videos on Decision Trees: https://www.youtube.com/watch?v=Pz6xX6rK5M4&list=PLBv09BD7ez_4_UoYeGrzvqveIR_USBEKD
4. 60min: My favorite youtube lecturer has this series: https://www.youtube.com/watch?v=p17C9q2M00Q&list=PLScpSunNAuBY8E5O2s_61jToIHuYKp_c7

# Lecture 4 - Measuring Distances

# Blogs:
1. 4 Distance Measures for Machine Learning: https://machinelearningmastery.com/distance-measures-for-machine-learning/
2. Short read: 3 Common Techniques of Similarity and Distance Measure in Machine Learning https://machinelearningknowledge.ai/3-common-techniques-similarity-distance-measure-machine-learning/
3. Mathematical description of Vector Norms: http://fourier.eng.hmc.edu/e161/lectures/algebra/node11.html
4. Great Read: Vector Norms and Inequalities with Python https://aaronschlegel.me/vector-norms-inequalities-python.html
5. Another Great Read: Matrix Norms and Inequalities with Python https://aaronschlegel.me/matrix-norms-inequalities-python.html
6. Beautiful description of Mahalaobis distances and its importance: https://www.machinelearningplus.com/statistics/mahalanobis-distance/
7. KL-divergence explained: https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained
8. Another example of KL-divergence: https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-understanding-kl-divergence-2b382ca2b2a8
9. Last: Importance of Distance Metrics in Machine Learning Modelling https://towardsdatascience.com/importance-of-distance-metrics-in-machine-learning-modelling-e51395ffe60d

## Videos:
1. (7min): Jupyter Notebook video on L0 and L1 distance measures: https://www.youtube.com/watch?v=-3Uq_FZkzpc
2. (7min): Visualizing norms as unit circles: https://www.youtube.com/watch?v=SXEYIGqXSxk
3. (6min): Short video on vector norms: https://www.youtube.com/watch?v=5fN2J8wYnfw
4. (10min): Mahalaobis Distance (weighted norms): https://www.youtube.com/watch?v=3IdvoI8O9hU
5. (42min): Classical Black-Board lecture on Norms (I do like those ...): https://www.youtube.com/watch?v=NcPUI7aPFhA
6. (13min): Levensthein Edit Distane: https://www.youtube.com/watch?v=Xxx0b7djCrs
7. (10min): KL-divergence: https://www.youtube.com/watch?v=LJwtEaP2xK
8. (12min): A Short Introduction to Entropy, Cross-Entropy and KL-Divergence - https://www.youtube.com/watch?v=ErfnhcEV1O8

# Lecture 5 - K-NN

## Videos:
1. (10min) Mathematical Monk (must watch!): https://www.youtube.com/watch?v=4ObVzTuFivY
2. (10min) Victor Lavrenk (another must watch!): https://www.youtube.com/watch?v=k_7gMp5wh5A
3. (50min) If you like the full blackboard lectures: https://www.youtube.com/watch?v=09mb78oiPkA
4. (10min) Handwritten example of boundaries in KNN: https://www.youtube.com/watch?v=JtBtVNtTXRQ

## Blogs
1. KNN in Python (don't use this code, we implement it from scratch): https://www.analyticsvidhya.com/blog/2018/03/introduction-k-neighbours-algorithm-clustering/
2. Wikipedia article is great: https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm
3. Beautiful Blog entry on KNN by Kevin Zakka: https://kevinzakka.github.io/2016/07/13/k-nearest-neighbor/
4. Free book to download and read (for indepth understanding): ntroduction to Statistical Learning with Applications in R, Chapters 2 and 3 - http://faculty.marshall.usc.edu/gareth-james/
5. A Detailed Introduction to K-Nearest Neighbor (KNN) Algorithm: https://saravananthirumuruganathan.wordpress.com/2010/05/17/a-detailed-introduction-to-k-nearest-neighbor-knn-algorithm/



# Lecture 6 - Linear and Polyomial Regression

## Videos
1. (~60 min) Video series on linear regression: https://www.youtube.com/watch?v=rVviNyIR-fI
2. (~60 min) Andrew Ng's full series on regression models: https://www.youtube.com/watch?v=kHwlB_j7Hkc&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=4
His series Nr. 3 is refresher on Linear Algebra, but then Lecture Nr. 4 is on linear regression multiple variables: https://www.youtube.com/watch?v=Q4GNLhRtZNc&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=18
3. (4min) Super short (but great) video on Polynomial Regression: https://www.youtube.com/watch?v=nGcMl03LPC0
4. (9min) Great video om Polynomial Regression: https://www.youtube.com/watch?v=QptI-vDle8Y
5. (13min) quite mathematical (but you'll need those equations) of linear regression: https://www.youtube.com/watch?v=K_EH2abOp00

# Blogs
1. Wikiedpia entry on Linear Regresion is indeed pretty good: https://en.wikipedia.org/wiki/Linear_regression
2. Wikipedia entry on Polynomial regression is indeed pretty good: https://en.wikipedia.org/wiki/Polynomial_regression
3. Regression is very common lecture topic in many different classes, not just ML. I found this very long and comprehensive lecture notes which basically comprise everything you need to know: https://stat.ethz.ch/education/semesters/ss2012/regression/RegressionEnglish.pdf

# Lecture 7 - Perceptron and Linear Discriminat Analysis

[Some history on the importance of perceptrons] (https://news.cornell.edu/stories/2019/09/professors-perceptron-paved-way-ai-60-years-too-soon)

## Videos
1. (13min)[Really nice Video Introduction](https://www.youtube.com/watch?v=4Gac5I64LM4)
2. (9min)[Another great introduction video](https://www.youtube.com/watch?v=5g0TPrxKK6o)
3. (45min) [Beautiful, quite easy series on perceptrons](https://www.youtube.com/watch?v=jbluHIgBmBo)

## Readings 
1. Very Comprehensive, but mathy overview of Perceptron and relations to other ML-algorithms:
https://www.pearsonhighered.com/assets/samplechapter/0/1/3/1/0131471392.pdf
2. [A nice (not super mathy) blog on perceptrons (I recommend!!!)[https://towardsdatascience.com/perceptron-learning-algorithm-d5db0deab975]

# Lecture 8 - Logistic Regression

# Lecture 9 - Gradient Descent for Optimizing the Loss Function

## Recap on Partial Derivatives and the Gradient of multi-dimensional functions
1. [What is a partical derivative ?](https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/partial-derivative-and-gradient-articles/a/introduction-to-partial-derivatives) and then [What is the Gradient](https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/partial-derivative-and-gradient-articles/a/the-gradient)
2. [A pretty nice sub-wiki which really covers everything you need to know](https://calculus.subwiki.org/wiki/Gradient_vector)
3. [This is deep learning blog, but everything said on Gradients is generally true and I really like this blog entry](https://towardsdatascience.com/step-by-step-the-math-behind-neural-networks-490dc1f3cfd9)
4. A bunch of examples where gradients are caculated: https://www.varsitytutors.com/linear_algebra-help/the-gradient
5. Linear Algebra and the Gradient (Deriving gradients with matrices, super important for ML): https://www.varsitytutors.com/linear_algebra-help/the-gradient
6. Understanding the math behind Gradient Descent: https://towardsdatascience.com/understanding-the-mathematics-behind-gradient-descent-dde5dc9be06e

## Understanding Gradient Descent Algorithm
1. (22min)[Step-by-Step introduction](https://www.youtube.com/watch?v=sDv4f4s2SB8)
2. Gradient Descent - THE MATH YOU SHOULD KNOW: https://www.youtube.com/watch?v=-p1ldISb90Q


### Book/ more scientific papers and reads:
1. Great (no, amazing) book on convex optimization (read chapter 1 - Introduction on a broad overview of Optimization not limited to Gradient Descent): https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf 
A few notes on this book: Every applied math/computer scientist grad student should have read this book (or at least large parts of it). (Convex) Optimization is such an important field which you'll find every where. Obviously, we can't teach it in this class, but I recommend you this book a lot!
2. An overview of gradient descent optimizationalgorithms (quite scientific but gives comprehensive overview): https://arxiv.org/pdf/1609.04747.pdf

# Lecture 9.2 Part - Regularization

## Occam’s Razor (preferring simpler models):
1. [Nice, basic video (without any math)] (https://www.youtube.com/watch?v=M5WDdvkFaDg)
2. [A bit more mathy video on Occam Razor for ML](https://www.youtube.com/watch?v=Q_AclBHCaUo)

### Blogs:
1. Intuition of Regularization (blog): https://towardsdatascience.com/intuitions-on-l1-and-l2-regularisation-235f2db4c261

### Videos
1. Andrew NG series on Regularization [Part1: The Problem Of Overfitting](https://www.youtube.com/watch?v=u73PU6Qwl1I), [Part 2: Lost Function]( https://www.youtube.com/watch?v=KvtGD37Rm5I ), [Part 3: Regularized Linear Regression] ( https://www.youtube.com/watch?v=qbvRdrd0yJ8 )
2. Lp regularization penalties; comparing L2 vs L1: https://www.youtube.com/watch?v=sO4ZirJh9ds
3. Josh Stormers regularization video: [Part 1](https://www.youtube.com/watch?v=Q81RR3yKn30), [Part 2](https://www.youtube.com/watch?v=NGf0voTMlcs)

