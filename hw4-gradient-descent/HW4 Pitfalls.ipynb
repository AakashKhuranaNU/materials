{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS349 HW4 (Gradient Descent) Pitfalls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's some stuff you might find useful for this assignment:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best order would work on the code is:\n",
    "- `regularization.py`\n",
    "- `loss.py`\n",
    "- `gradient_descent.py`\n",
    "- `multiclass_gradient_descent.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Regularization\n",
    "I've found that many students are quite confused with the backward pass of a regularization. The best way to explain it is that regularization is to more or less smoothing out one's loss function.\n",
    "The forward pass is simply regularizing each element in the array to the norm. For example, for L1: the regularization of `x` is simply `|x|`. For L2, `x` becomes `.5 * x^2`. Forward pass would be summing all `x` in the given array multiplying by `self.reg_param`, and returning.\n",
    "\n",
    "The backward pass is applying a derivative to the regularized version of each point in the given array except for the last bias term. For L1: `(|x|)' = either -1, 1, or 0` depending on the sign on x. Best to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backward = np.sign(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "making the last term `0` (derivative of any constant/bias is 0), and returning.\n",
    "**Hint for L2:** what is the derivative of `.5x^2`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Loss\n",
    "Forward loss is the application of calculating the loss value on a set.\n",
    "Add `self.regularization.forward(w)` to the calculated loss before returning.\n",
    "Backward loss calculates the gradient (multi-dimensional derivative) of the loss function.\n",
    "Add `self.regularization.backward(w)` to the calculated gradient array before returning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Gradient Descent\n",
    "You will implement the Gradient Descent class fit, predict and confidence functions.\n",
    "\n",
    "Fit:\n",
    "This will be where most of the coding is done. On a high level, you're simply:\n",
    "- calculating the loss that the current weight parameters give you, \n",
    "- calculating the gradient of the loss at that current point\n",
    "- shifting the weight parameters in the direction opposite of the gradient by `learning_rate` in order to lower the loss value,\n",
    "- continuing this process until you reach `max_iter`, or your change in loss value becomes negligible.\n",
    "\n",
    "In your function, you should keep track of **previous_weight_params** and **current_weight_params,** and make use of `self.loss.forward()` in your code such as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#previous_loss is your old loss\n",
    "current_loss = self.loss.forward(features, self.model, targets)\n",
    "calculated_loss_change = np.abs(previous_loss - current_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict:\n",
    "Hint: Should be only a few lines (1 or 2) and makes use of `self.confidence()`. Takes the sign value of the confidence output.\n",
    "\n",
    "Confidence:\n",
    "Also 1-2 lines, making use of `np.dot()` to a modified `features` array to which you \"stacked\" on top of `np.ones((features.shape[0], 1))`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Multiclass Gradient Descent\n",
    "I don't have much to say here, because you are simply creating **c** gradient descent models and training each of them to their own class.\n",
    "It is recommended to first find the labels/names of each class, which you can easily do so in your `fit` function by doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.classes = np.unique(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then in a `for` loop, create **c** `GradientDescent` classes and train them to each of the classes in `self.classes`.\n",
    "In `predict`, run another `for` loop to calculate `self.model[i].confidence(features)`, and afterwards, it is recommended to use `np.argmax(your_predicted_values, axis=1)` to find the index of each class that's predicted."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
