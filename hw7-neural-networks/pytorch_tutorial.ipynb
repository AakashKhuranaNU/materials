{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Tutorial\n",
    "\n",
    "This Jupyter Notebook will cover the PyTorch functions that you will find the most useful in Homework 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Layers\n",
    "The following are neural network layers. In Homework 7, we will be using `nn.Linear`, `nn.Conv2d`, and `nn.MaxPool2d`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The module torch.nn contains network layers\n",
    "\n",
    "# creates a linear layer with 128 input dimensions and 64 output dimensions. \n",
    "in_dim, out_dim = 128, 64\n",
    "linear = nn.Linear(in_dim, out_dim)\n",
    "\n",
    "# creates a 2d convolutional layer with the following params\n",
    "in_channels = 4\n",
    "out_channels = 12\n",
    "kernel_size = 5\n",
    "stride = 2\n",
    "dilation = 2\n",
    "conv2d = nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, dilation=dilation)\n",
    "\n",
    "# creates a max pool layer\n",
    "kernel_size = 5\n",
    "max_pool  = nn.MaxPool2d(kernel_size=kernel_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional layers can get pretty complex, you can find some information about some of the params here:\n",
    "\n",
    "https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can chain layers together using nn.Sequential\n",
    "seq_layer = nn.Sequential(\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 2),\n",
    "    nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions\n",
    "\n",
    "Activation functions are also from Module `torch.nn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rectified Linear Unit (ReLU)\n",
    "relu = nn.ReLU()\n",
    "\n",
    "# Sigmoid\n",
    "sig = nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Layers and Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 64])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After initializing a layer or activation function, you can pass input to it simply by using __call__()\n",
    "# This is true for all classes that implement nn.Module\n",
    "\n",
    "# Create 30 samples with dimension 128\n",
    "x = torch.rand(30, 128)\n",
    "out = linear(x)\n",
    "out = relu(out)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 2])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_out = seq_layer(x)\n",
    "seq_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "We will only be using Cross Entropy loss function for this homework. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.2204, grad_fn=<NllLossBackward>)\n",
      "tensor(4.2204, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "pred = out\n",
    "targets = torch.randint(0, 64, (30,))\n",
    "\n",
    "# For Cross Entropy, the predicted classes are the one-hot encoded in pred. \n",
    "# And targets[i] is the target for feature row i\n",
    "\n",
    "# You can run cross entropy in two ways\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "loss = loss_func(pred, targets)\n",
    "print(loss)\n",
    "# or\n",
    "loss = F.cross_entropy(pred, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "\n",
    "The optimizer updates the parameters of a network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SGD Optimizer to update the parameters of a linear layer with learning rate of .001\n",
    "optimizer = optim.SGD(linear.parameters(), lr=.001)\n",
    "\n",
    "# To update the gradients of the optimizer, after taking calculating the loss\n",
    "loss.backward()\n",
    "# To update the parameters according to a gradient\n",
    "optimizer.step()\n",
    "# To reset gradients\n",
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.tensor.numpy\n",
    "# Convert tensor to numpy array\n",
    "np_x = x.numpy()\n",
    "\n",
    "# For tensors involved with gradient descent, such as network parameters or network outputs, \n",
    "# you will need to do .detach() first\n",
    "np_out = out.detach().numpy()\n",
    "\n",
    "# Convert numpy to torch\n",
    "ones = np.ones(100)\n",
    "ones = torch.tensor(ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.rand\n",
    "# random floats from 0 to 1 with shape (a, b, c)\n",
    "a = 5\n",
    "b = 10\n",
    "c = 30\n",
    "r = torch.rand(5, 10, 30)\n",
    "\n",
    "# torch.randint\n",
    "# random integers from 0 to n-1 with shape (a, b, c)\n",
    "n = 100\n",
    "ri = torch.randint(100, (a, b, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 100])\n",
      "torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "# torch.tensor.flatten\n",
    "# Flatten a n-D tensor\n",
    "r = torch.rand(100, 100)\n",
    "print(r.shape)\n",
    "r = r.flatten()\n",
    "print(r.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2, 1])\n",
      "torch.Size([1, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "# torch.tensor.permute\n",
    "# Rerange the axes of a tensor\n",
    "# For example, this transforms a tensor with shape (4, 2, 1) to (1, 4, 2)\n",
    "x = torch.zeros(4, 2, 1)\n",
    "print(x.shape)\n",
    "permute_x = x.permute(2, 0, 1)\n",
    "print(permute_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
